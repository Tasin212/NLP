{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebd3135e-c452-4de4-ba72-e197ae8e8413",
   "metadata": {},
   "source": [
    "# Removing Tags"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc68d33d-3155-4905-9bcc-23fa18c8361c",
   "metadata": {},
   "source": [
    "In order to do that normally we use regex. We can write code for regex and to quickly find out the regex we go to this website https://regex101.com/ try out different regexes for our string. We just simply write down our string in the Test String section and then try out regex for a particular pattern.\n",
    "This is the regex : <.*?> to remove all HTML tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aefb678-0660-4220-be3f-56dffce8bc49",
   "metadata": {},
   "source": [
    "# Removing Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c2adff1-ea50-4426-981b-7aa36d5a9149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out my notebook  \n",
      " Check out my notebook  \n",
      " Google search here  \n",
      " For notebook click  to search check \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_url(text):\n",
    "    exp = r'https?://\\S+|www\\.\\S+'\n",
    "    result = re.sub(exp, r'', text)\n",
    "    return result\n",
    "\n",
    "text1 = 'Check out my notebook https://www.kaggle.com/campusx/notebook8223fc1abb'\n",
    "text2 = 'Check out my notebook http://ww.kaggle.com/campusx/notebook8223fc1abb'\n",
    "text3 = 'Google search here www.google.com'\n",
    "text4 = 'For notebook click https://www.kaggle.com/campusx/notebook8223fcabb to search check www.google.com'\n",
    "\n",
    "result1 = remove_url(text1)\n",
    "result2 = remove_url(text2)\n",
    "result3 = remove_url(text3)\n",
    "result4 = remove_url(text4)\n",
    "\n",
    "print(result1, '\\n', result2, '\\n', result3, '\\n', result4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58b27c3-8c06-4638-940a-bb12831b617f",
   "metadata": {},
   "source": [
    "# Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cadf4abf-30f2-4e4a-8168-2642ff19695a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'string with Punction right'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation) #it will print all punctuation\n",
    "\n",
    "exclue = string.punctuation\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    for char in exclue:\n",
    "        text = text.replace(char,'')\n",
    "    return text\n",
    "\n",
    "text = 'string. with. Punction? right?!'\n",
    "result = remove_punctuation(text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7948edf5-c898-4027-9bcb-e94fc0805bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Optimized method\n",
      "string with Punction right\n"
     ]
    }
   ],
   "source": [
    "# now assume if we have 50,000 rows and we are using this fucntion to remove punction, it will take time1 * 50,000 and it's really time consuming\n",
    "# so the optimzed function is \n",
    "print('\\nFor Optimized method')\n",
    "def remove_punctuation2(text):\n",
    "    return text.translate(str.maketrans('', '', exclue))\n",
    "\n",
    "print(remove_punctuation2(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e496e604-e7cf-4942-b44c-d2ff14566155",
   "metadata": {},
   "source": [
    "# Chat word treatment\n",
    "Means if the dataset is something like conversational in that case dataset might contain some short words like GN - good night ; asap ; lmao etc in that case we have to give the full form of those words (eg: GN - Good Night) using a dictionary.\n",
    "https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt In this link you will find more than 100 words like that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de2b474d-28e1-4022-a872-1131f7c4174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now I just took a few words from the above link\n",
    "chat_words = {\n",
    "    \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
    "    \"ADIH\": \"Another Day In Hell\",\n",
    "    \"AFK\": \"Away From Keyboard\",\n",
    "    \"AFAIK\": \"As Far As I Know\",\n",
    "    \"ASAP\": \"As Soon As Possible\",\n",
    "    \"ASL\": \"Age, Sex, Location\",\n",
    "    \"ATK\": \"At The Keyboard\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "063907f0-ecd0-4290-b31d-0f15fe132ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversation(text):\n",
    "    new_text = []\n",
    "    for w in text.split(): # we are splitting the upcoming text into words\n",
    "        if w.upper() in chat_words:\n",
    "            new_text.append(chat_words[w.upper()]) # means if words in the chat_words then we'r extracting the meaning and appending to the list\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f869b7d-3556-4602-b4e4-30d392236e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Send me those files As Soon As Possible'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_conversation('Send me those files ASAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9354a14-9d25-4ff1-8efc-8a9a71853ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'So new bee whats your Age, Sex, Location'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_conversation('So new bee whats your ASL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c81c804-45ad-43ef-9ff6-02b8b36e11ef",
   "metadata": {},
   "source": [
    "# Spelling Correction \n",
    "Eg: Please read the note book, and also like the notebook\n",
    "In this sentence when tokenize will be performed  it will create some issue due to the different notebook spelling while correct spelling is this : notebook , they should be together not individual. Now to tackle with there are multiple libraries such as NLTK, Spacy, textblob etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "066e8f67-d558-41a0-88a9-cf26fa72d97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\labib\\anaconda3\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\labib\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\labib\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\labib\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\labib\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\labib\\anaconda3\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "   ---------------------------------------- 0.0/624.3 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 262.1/624.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 624.3/624.3 kB 5.0 MB/s eta 0:00:00\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7b0a103-bab6-43be-971f-d257f2df6568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07113dcd-ee3f-4399-b149-281ee0fa9765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Certain conditions during several generations are modified in the same manner'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_text = 'Ceertain conditonas duriing severel generations aree modifiad in the same mannar'\n",
    "textblb = TextBlob(incorrect_text)\n",
    "textblb.correct().string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245b2be5-3e94-4812-9792-26761aaca705",
   "metadata": {},
   "source": [
    "# Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b00ba0a-0a83-41d9-b434-334d8bf92098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "189d0328-9527-422a-923f-98f76a485f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    words = [word for word in text.split() if word.lower() not in stop_words]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87963e39-264b-454b-9e57-03642b470e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probably all-time favourite movie, story selflessness, sacrifice dedication noble cause'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords('probably my all-time favourite movie, a story of selflessness, sacrifice and dedication to a noble cause')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd788e62-ca35-41b7-b2b3-10fc89012f3d",
   "metadata": {},
   "source": [
    ".\n",
    "Now let's say you want to apply the remove_stopwords fucntion for bigger dataset it can be tabular dataset where there might be a cols name review. so it apply it on the review col simpley df['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef39115-f4b6-4f8f-b784-4fb50685aaaa",
   "metadata": {},
   "source": [
    "# Handling Emojis\n",
    "In some cases we may find emojis but the problem is model doesn’t understand emojis, so we can do 2 things \n",
    "Remove Emojis or\n",
    "Replace (eg: for happy face emoji –. happy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2899f04d-8109-4d2f-872c-a511c84382da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\" # EMOTICONS\n",
    "                               u\"\\U0001F300-\\U0001F5FF\" # SYMBOLS & PICTOGRAPHS\n",
    "                               u\"\\U0001F680-\\U0001F6FF\" # TRANSPORT & MAP SYMBOLS\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\" # FLAGS (IOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cbc8d907-28f4-4f2f-8cbd-2ec96a6ad799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loved the movie, it was '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji(\"Loved the movie, it was 😅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c9bb4a4c-4e8a-409a-abb1-140f47a5e268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
      "   ---------------------------------------- 0.0/590.6 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 262.1/590.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 590.6/590.6 kB 3.8 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b84fe39b-3eaf-4f6f-861b-6b432ad5d0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is :fire:, and :grinning_face_with_sweat: and :backhand_index_pointing_right: and :light_bulb:\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "print(emoji.demojize('Python is 🔥, and 😅 and 👉 and 💡'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e12655-a965-45eb-9380-83bb022bf78d",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "There are few ways we can do tokenization.\n",
    "1. Using Split function, it works for basic text.\n",
    "2. Using Regex\n",
    "3. NLTK\n",
    "4. Spacy (recommended)\n",
    "   Also which technique will be used based on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeba6b72-59b8-4217-847c-86762724a572",
   "metadata": {},
   "source": [
    "### Using Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1eff7892-e40d-493e-9f58-8df19f84ba4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'dhaka']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = \"I am going to dhaka\"\n",
    "sent1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a7b550f-cb3a-4dcf-9d3c-956222db8d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am going to dhaka',\n",
       " ' I will stay there for 3 das',\n",
       " \" Let's hope for the best\"]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "sent2 = 'I am going to dhaka. I will stay there for 3 das. Let\\'s hope for the best'\n",
    "sent2.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "be177158-a111-40b8-933e-cb8695c6b801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where do think I should go? I have a 3 day holiday']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# problem with split, in this case it couldn't tokenize the sentence\n",
    "sent3 = 'Where do think I should go? I have a 3 day holiday'\n",
    "sent3.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eba4449f-aa0c-4683-8d42-0a799b47f9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'dhaka!']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# problem with word tokenization (here it spliteed dhaka! not dhaka)\n",
    "sent4 = 'I am going to dhaka!'\n",
    "sent4.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7ef16b-32bc-4186-9d61-ef75d3402f5f",
   "metadata": {},
   "source": [
    "### Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "91e7c29d-a7b5-4c0a-900d-b4391d744f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'dhaka']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sent3 = 'I am going to dhaka!'\n",
    "tokens = re.findall(\"[\\w]+\", sent3)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f02cb4-8771-4a43-845a-abac7a5cfaf6",
   "metadata": {},
   "source": [
    "### NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e1c841e5-95ca-469c-9d1f-2059b8f853dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f0ba1d1-bf05-470f-865c-b8e0b383d123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'visit', 'dhaka', '!']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = 'I am going to visit dhaka!'\n",
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2338d7e6-7648-4f48-b33b-30f7045a262b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['orem Ipsum is simply dummy text of the printing and typesetting industry.',\n",
       " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2 = '''orem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.\n",
    "'''\n",
    "sent_tokenize(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "19a67eb0-45a0-4308-a098-37c499d38197",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent3 = 'I have a Ph.D in A.I'\n",
    "sent4 = \"We're here to help! mail us at nks@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a333a521-01b9-45d6-accb-15dde13068b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'have', 'a', 'Ph.D', 'in', 'A.I']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3a58889e-5d31-4849-b5eb-260cf6fec654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " \"'re\",\n",
       " 'here',\n",
       " 'to',\n",
       " 'help',\n",
       " '!',\n",
       " 'mail',\n",
       " 'us',\n",
       " 'at',\n",
       " 'nks',\n",
       " '@',\n",
       " 'gmail.com']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#but in this case NLTK failed it breaks the gmail id\n",
    "word_tokenize(sent4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031af356-929e-4b2d-a1b2-eb1ffbc72c4e",
   "metadata": {},
   "source": [
    "### Spacy (proper one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "75b89f1a-af61-4d3a-bf51-24c3ff7f7253",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp313-cp313-win_amd64.whl.metadata (28 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.13-cp313-cp313-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp313-cp313-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.10-cp313-cp313-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp313-cp313-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\labib\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\labib\\anaconda3\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\labib\\anaconda3\\lib\\site-packages (from spacy) (2.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\labib\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\labib\\anaconda3\\lib\\site-packages (from spacy) (2.10.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\labib\\anaconda3\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\labib\\anaconda3\\lib\\site-packages (from spacy) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\labib\\anaconda3\\lib\\site-packages (from spacy) (24.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\labib\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\labib\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\labib\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\labib\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\labib\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\labib\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\labib\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0-cp313-cp313-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\labib\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\labib\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.22.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.3.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\labib\\anaconda3\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.3.1-cp313-cp313-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\labib\\anaconda3\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Downloading spacy-3.8.7-cp313-cp313-win_amd64.whl (13.9 MB)\n",
      "   ---------------------------------------- 0.0/13.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/13.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/13.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/13.9 MB 1.2 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 1.0/13.9 MB 1.4 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.3/13.9 MB 1.4 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 1.8/13.9 MB 1.6 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 2.6/13.9 MB 1.9 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 3.4/13.9 MB 2.2 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 3.9/13.9 MB 2.3 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 4.5/13.9 MB 2.3 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 5.0/13.9 MB 2.3 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 5.5/13.9 MB 2.4 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 6.0/13.9 MB 2.4 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 6.6/13.9 MB 2.4 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 7.1/13.9 MB 2.4 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 7.6/13.9 MB 2.4 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 8.1/13.9 MB 2.4 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 8.7/13.9 MB 2.4 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 9.2/13.9 MB 2.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 9.7/13.9 MB 2.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 10.2/13.9 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 10.7/13.9 MB 2.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 11.3/13.9 MB 2.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 11.8/13.9 MB 2.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.3/13.9 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.8/13.9 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.4/13.9 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.9/13.9 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.9/13.9 MB 2.4 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp313-cp313-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.13-cp313-cp313-win_amd64.whl (24 kB)\n",
      "Downloading preshed-3.0.10-cp313-cp313-win_amd64.whl (115 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp313-cp313-win_amd64.whl (630 kB)\n",
      "   ---------------------------------------- 0.0/630.6 kB ? eta -:--:--\n",
      "   --------------------------------- ------ 524.3/630.6 kB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 630.6/630.6 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.6-cp313-cp313-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 0.8/1.7 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.3/1.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 2.1 MB/s eta 0:00:00\n",
      "Downloading blis-1.3.0-cp313-cp313-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/6.3 MB 2.3 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 1.6/6.3 MB 2.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.1/6.3 MB 2.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 2.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.4/6.3 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 3.9/6.3 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.7/6.3 MB 2.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.2/6.3 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.3 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 2.6 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.22.0-py3-none-any.whl (61 kB)\n",
      "Downloading smart_open-7.3.1-py3-none-any.whl (61 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/5.4 MB 3.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.3/5.4 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 1.8/5.4 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 2.6/5.4 MB 3.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.1/5.4 MB 3.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.7/5.4 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.2/5.4 MB 2.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 4.7/5.4 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.2/5.4 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 2.7 MB/s eta 0:00:00\n",
      "Downloading marisa_trie-1.3.1-cp313-cp313-win_amd64.whl (139 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, langcodes, confection, weasel, thinc, spacy\n",
      "\n",
      "   ------ ---------------------------------  3/18 [spacy-legacy]\n",
      "   --------------- ------------------------  7/18 [cloudpathlib]\n",
      "   -------------------- -------------------  9/18 [blis]\n",
      "   ---------------------- ----------------- 10/18 [srsly]\n",
      "   ---------------------- ----------------- 10/18 [srsly]\n",
      "   ---------------------- ----------------- 10/18 [srsly]\n",
      "   ---------------------- ----------------- 10/18 [srsly]\n",
      "   ---------------------- ----------------- 10/18 [srsly]\n",
      "   ---------------------- ----------------- 10/18 [srsly]\n",
      "   ---------------------- ----------------- 10/18 [srsly]\n",
      "   -------------------------- ------------- 12/18 [language-data]\n",
      "   -------------------------- ------------- 12/18 [language-data]\n",
      "   -------------------------- ------------- 12/18 [language-data]\n",
      "   -------------------------- ------------- 12/18 [language-data]\n",
      "   -------------------------- ------------- 12/18 [language-data]\n",
      "   -------------------------- ------------- 12/18 [language-data]\n",
      "   -------------------------- ------------- 12/18 [language-data]\n",
      "   ---------------------------- ----------- 13/18 [langcodes]\n",
      "   ------------------------------- -------- 14/18 [confection]\n",
      "   --------------------------------- ------ 15/18 [weasel]\n",
      "   ----------------------------------- ---- 16/18 [thinc]\n",
      "   ----------------------------------- ---- 16/18 [thinc]\n",
      "   ----------------------------------- ---- 16/18 [thinc]\n",
      "   ----------------------------------- ---- 16/18 [thinc]\n",
      "   ----------------------------------- ---- 16/18 [thinc]\n",
      "   ----------------------------------- ---- 16/18 [thinc]\n",
      "   ----------------------------------- ---- 16/18 [thinc]\n",
      "   ----------------------------------- ---- 16/18 [thinc]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ------------------------------------- -- 17/18 [spacy]\n",
      "   ---------------------------------------- 18/18 [spacy]\n",
      "\n",
      "Successfully installed blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.22.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.3.1 murmurhash-1.0.13 preshed-3.0.10 smart-open-7.3.1 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "057f0a96-1bc2-4775-a944-072aa78ae6d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 5.4 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 3.1 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 2.1/12.8 MB 2.8 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 2.4/12.8 MB 3.1 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 3.1/12.8 MB 3.0 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 3.9/12.8 MB 2.9 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 4.5/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 5.0/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 2.8 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 6.0/12.8 MB 2.7 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 6.6/12.8 MB 2.7 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 6.8/12.8 MB 2.7 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 2.6 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.9/12.8 MB 2.6 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.4/12.8 MB 2.6 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.9/12.8 MB 2.5 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 2.5 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 10.0/12.8 MB 2.5 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 10.5/12.8 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 2.5 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.5/12.8 MB 2.5 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 2.5 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8864c0ff-9585-4320-b74c-e4240ba55f40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ed04b9a7-e967-43b9-a273-59a5eeec021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for spacy we have to convert sentences to document\n",
    "doc1 = nlp(sent1)\n",
    "doc2 = nlp(sent2)\n",
    "doc3 = nlp(sent3)\n",
    "doc4 = nlp(sent4) # email one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "235d2001-1428-4e1b-9bdd-de0d510886b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "am\n",
      "going\n",
      "to\n",
      "visit\n",
      "dhaka\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "011b3684-fdc0-4964-af4a-997e1d1d31a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "have\n",
      "a\n",
      "Ph\n",
      ".\n",
      "D\n",
      "in\n",
      "A.I\n"
     ]
    }
   ],
   "source": [
    "for token in doc3: #in this case NLTK perform well as in here Ph.D couldn't splitted properly\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "92aaaa80-0476-4436-a39a-8bbc8e33e94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We\n",
      "'re\n",
      "here\n",
      "to\n",
      "help\n",
      "!\n",
      "mail\n",
      "us\n",
      "at\n",
      "nks@gmail.com\n"
     ]
    }
   ],
   "source": [
    "for token in doc4: #here email splitted properly\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82e755e-2031-416b-adba-ec350dc8f948",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "In grammar, inflection is the modification of a word to express different grammatical categories such as tense, case, voice, aspect, person, number, gender and mood.\n",
    "eg: Walk , to this word we can add prefix and suffix : Walking, Walked, walks similary for <Do> Undoable here Do and Walk is the root word\n",
    "\n",
    "Stemming is the process of reducing inflection in words to their root form such as mapping group of words to the same stem even if the stem itself is not valid word in the language. (simpley if again convert the inflected word to their root form)\n",
    "\n",
    "We wil be using NLTK library for stemming, here we will be using Porter Stemmer(for english) and Snow ball stemmer usually used for other language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "24d9b6d0-7910-4d3d-b323-74a0fa869c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9e9f07b3-9f2c-4e79-997a-df322d614f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "00e02e0f-baf4-46cc-8fd1-fa76a4ccc261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 'walk walks walking walked'\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a8d6ae89-1d4c-4ef3-80f9-a290f834ec24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i had alway thought jack gisburn rather a cheap genius--though a good fellow enough--so it wa no great surpris to me to hear that, in the height of hi glory, he had drop hi painting, marri a rich widow, and establish himself in a villa on the riviera. (though i rather thought it would have been rome or florence.) \"the height of hi glory\"--that wa what the women call it. i can hear mrs. gideon thwing--hi last chicago sitter--deplor hi unaccount abdication. \"of cours it\\' go to send the valu of my pictur \\'way up; but i don\\'t think of that, mr. rickham--th loss to arrt is all i think of.\" the word, on mrs. thwing\\' lips, multipli it _rs_ as though they were reflect in an endless vista of mirrors. and it wa not onli the mrs. thwing who mourned. had not the exquisit hermia croft, at the last grafton galleri show, stop me befor gisburn\\' \"moon-dancers\" to say, with tear in her eyes: \"we shall not look upon it like again\"? well!--even through the prism of hermia\\' tear i felt abl to face the fact with equanimity. poor jack gisburn! the women had made him--it wa fit that they should mourn him. among hi own sex fewer regret were heard, and in hi own trade hardli a murmur. profession jealousy? perhaps. if it were, the honour of the craft wa vindic by littl claud nutley, who, in all good faith, brought out in the burlington a veri handsom \"obituary\" on jack--on of those showi articl stock with random technic that i have heard (i won\\'t say by whom) compar to gisburn\\' painting. and so--hi resolv be appar irrevocable--th discuss gradual die out, and, as mrs. thwing had predicted, the price of \"gisburns\" went up. it wa not till three year later that, in the cours of a few weeks\\' idl on the riviera, it suddenli occur to me to wonder whi gisburn had given up hi painting. on reflection, it realli wa a tempt problem. to accus hi wife would have been too easy--hi fair sitter had been deni the solac of say that mrs. gisburn had \"drag him down.\" for mrs. gisburn--a such--had not exist till nearli a year after jack\\' resolv had been taken. it might be that he had marri her--sinc he like hi ease--becaus he didn\\'t want to go on painting; but it would have been hard to prove that he had given up hi paint becaus he had marri her. of course, if she had not drag him down, she had equally, as miss croft contended, fail to \"lift him up\"--sh had not led him back to the easel. to put the brush into hi hand again--what a vocat for a wife! but mrs. gisburn appear to have disdain it--and i felt it might be interest to find out why.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text ='''I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
    "\n",
    "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it's going to send the value of my picture 'way up; but I don't think of that, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing's lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn's \"Moon-dancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?\n",
    "\n",
    "Well!--even through the prism of Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own trade hardly a murmur. Professional jealousy? Perhaps. If it were, the honour of the craft was vindicated by little Claude Nutley, who, in all good faith, brought out in the Burlington a very handsome \"obituary\" on Jack--one of those showy articles stocked with random technicalities that I have heard (I won't say by whom) compared to Gisburn's painting. And so--his resolve being apparently irrevocable--the discussion gradually died out, and, as Mrs. Thwing had predicted, the price of \"Gisburns\" went up.\n",
    "\n",
    "It was not till three years later that, in the course of a few weeks' idling on the Riviera, it suddenly occurred to me to wonder why Gisburn had given up his painting. On reflection, it really was a tempting problem. To accuse his wife would have been too easy--his fair sitters had been denied the solace of saying that Mrs. Gisburn had \"dragged him down.\" For Mrs. Gisburn--as such--had not existed till nearly a year after Jack's resolve had been taken. It might be that he had married her--since he liked his ease--because he didn't want to go on painting; but it would have been hard to prove that he had given up his painting because he had married her.\n",
    "\n",
    "Of course, if she had not dragged him down, she had equally, as Miss Croft contended, failed to \"lift him up\"--she had not led him back to the easel. To put the brush into his hand again--what a vocation for a wife! But Mrs. Gisburn appeared to have disdained it--and I felt it might be interesting to find out why.\n",
    "'''\n",
    "\n",
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "783cf769-7e76-4baa-8475-8489d9fb845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case of stemming sometimes the root word isn't a english word like < probably > stem word is < probabal > so stemming isn't right format to \n",
    "# show the user, correct format is lemmatization, lemmatization is slow compared to stemming. \n",
    "# if you don't want to show the output we should use stemming, if we want to show the output to user then we have to do lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f1ee4c-55f6-4961-b0be-8df101ea8efe",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Lemmatization, unlike stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In lemmatization root word is called Lemma. A lemma(plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words.\n",
    "\n",
    "Here we wil be using WordNet which is a lexical dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3c719414-1731-45e9-936f-c63b08f788be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Labib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, download the required NLTK data\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8b4f7917-f042-4511-9229-ac2c2adfcfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2d91a367-8de0-43b2-9a5a-600b1a62cff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the sun\"\n",
    "punctuations = \"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0dc4890a-11a6-4a47-ae50-b8eca7e0864c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 ha                  \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swimming            \n",
      "after               after               \n",
      "playing             playing             \n",
      "long                long                \n",
      "hours               hour                \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "sun                 sun                 \n"
     ]
    }
   ],
   "source": [
    "print(\"{0:20}{1:20}\".format(\"Word\", \"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print(\"{0:20}{1:20}\".format(word, wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "08a9b8de-37a5-4b70-9fc1-f2825b060170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "sun                 sun                 \n"
     ]
    }
   ],
   "source": [
    "# now notice one thing still is not appropriate format, we need to add another parameter (pos='v'))\n",
    "print(\"{0:20}{1:20}\".format(\"Word\", \"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print(\"{0:20}{1:20}\".format(word, wordnet_lemmatizer.lemmatize(word, pos='v')))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
