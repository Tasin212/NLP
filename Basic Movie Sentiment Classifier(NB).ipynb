{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1318d7d-4be5-4878-bff8-ebd5f460a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords, names\n",
    "from nltk import pos_tag, word_tokenize, sent_tokenize\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy\n",
    "from statistics import mean\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2449b04d-407a-4c49-b438-b5c050865140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Labib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Labib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Labib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\Labib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Labib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Labib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download([\n",
    "    'movie_reviews',\n",
    "    'punkt',\n",
    "    'stopwords',\n",
    "    'names',\n",
    "    'averaged_perceptron_tagger',\n",
    "    'vader_lexicon'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc597a13-e721-406d-924b-5dc7c7e5cdb3",
   "metadata": {},
   "source": [
    "# Step 2: Split Data First (No Leakage!)\n",
    "We split before learning anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69049585-ac21-447f-9fe1-9c067563685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all review IDs\n",
    "pos_ids = movie_reviews.fileids(\"pos\")\n",
    "neg_ids = movie_reviews.fileids(\"neg\")\n",
    "\n",
    "# Shuffle them\n",
    "shuffle(pos_ids)\n",
    "shuffle(neg_ids)\n",
    "\n",
    "# Define split point (80% train)\n",
    "train_ratio = 0.8\n",
    "train_cutoff_pos = int(len(pos_ids) * train_ratio)\n",
    "train_cutoff_neg = int(len(neg_ids) * train_ratio)\n",
    "\n",
    "# Training IDs (80%)\n",
    "train_pos_ids = pos_ids[:train_cutoff_pos]   # 4,000\n",
    "train_neg_ids = neg_ids[:train_cutoff_neg]   # 4,000\n",
    "\n",
    "# Test IDs (20%)\n",
    "test_pos_ids = pos_ids[train_cutoff_pos:]    # 1,000\n",
    "test_neg_ids = neg_ids[train_cutoff_neg:]    # 1,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76af2488-0301-4196-b29c-498f3ee82dd2",
   "metadata": {},
   "source": [
    "# Step 3: Build top_100_positive from Training Data Only\n",
    "This is the key fix â€” no leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05adbf89-5634-444b-b82e-7318ffa85984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define unwanted words\n",
    "unwanted = set(stopwords.words(\"english\"))\n",
    "unwanted.update([name.lower() for name in names.words()])\n",
    "\n",
    "def skip_unwanted(pos_tuple):\n",
    "    word, tag = pos_tuple\n",
    "    if not word.isalpha() or word.lower() in unwanted:\n",
    "        return False\n",
    "    if tag.startswith(\"NN\"):  # Skip nouns\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Extract words from training positive reviews only\n",
    "train_words_pos = [\n",
    "    word for word, tag in filter(\n",
    "        skip_unwanted,\n",
    "        pos_tag(movie_reviews.words(train_pos_ids))\n",
    "    )\n",
    "]\n",
    "\n",
    "# Build frequency distribution\n",
    "positive_fd = nltk.FreqDist(train_words_pos)\n",
    "\n",
    "# Top 100 positive words (lowercase!)\n",
    "top_100_positive = {word.lower() for word, count in positive_fd.most_common(100)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7380001-de69-4665-a262-cab650ad10e6",
   "metadata": {},
   "source": [
    "# Step : 4 Define extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68c1b0e4-84eb-43bd-ac6b-95e06c2cb55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def extract_features(text):\n",
    "    features = {}\n",
    "    wordcount = 0\n",
    "    compound_scores = []\n",
    "    positive_scores = []\n",
    "\n",
    "    for sentence in sent_tokenize(text):\n",
    "        for word in word_tokenize(sentence):\n",
    "            if word.lower() in top_100_positive:\n",
    "                wordcount += 1\n",
    "        compound_scores.append(sia.polarity_scores(sentence)[\"compound\"])\n",
    "        positive_scores.append(sia.polarity_scores(sentence)[\"pos\"])\n",
    "\n",
    "    features[\"mean_compound\"] = mean(compound_scores) + 1\n",
    "    features[\"mean_positive\"] = mean(positive_scores)\n",
    "    features[\"wordcount\"] = wordcount\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f835d1-c9c7-4f54-8a39-b64ed583b45a",
   "metadata": {},
   "source": [
    "# ðŸ”§ Step 5: Build Features for Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faa966a4-7f5d-4da7-9b33-07d0f84b1328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training features\n",
    "train_features = []\n",
    "\n",
    "# Add positive training reviews\n",
    "for review_id in train_pos_ids:\n",
    "    text = movie_reviews.raw(review_id)\n",
    "    feats = extract_features(text)\n",
    "    train_features.append((feats, \"pos\"))\n",
    "\n",
    "# Add negative training reviews\n",
    "for review_id in train_neg_ids:\n",
    "    text = movie_reviews.raw(review_id)\n",
    "    feats = extract_features(text)\n",
    "    train_features.append((feats, \"neg\"))\n",
    "\n",
    "# Shuffle training data\n",
    "shuffle(train_features)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e51707b5-97a0-45a0-91d8-e2f43598404f",
   "metadata": {},
   "source": [
    "Now build test features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d20d7835-3cee-4b90-a19e-95253143e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test features\n",
    "test_features = []\n",
    "\n",
    "for review_id in test_pos_ids:\n",
    "    text = movie_reviews.raw(review_id)\n",
    "    feats = extract_features(text)\n",
    "    test_features.append((feats, \"pos\"))\n",
    "\n",
    "for review_id in test_neg_ids:\n",
    "    text = movie_reviews.raw(review_id)\n",
    "    feats = extract_features(text)\n",
    "    test_features.append((feats, \"neg\"))\n",
    "\n",
    "# No need to shuffle test set â€” weâ€™re just evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf47dbc7-8dce-4bfb-a61c-a5cbf95bb164",
   "metadata": {},
   "source": [
    "# ðŸ”§ Step 6: Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "172ab9cc-296f-4b81-ad5b-cf264abd80f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fair Accuracy: 0.458\n"
     ]
    }
   ],
   "source": [
    "# Train on training features\n",
    "classifier = NaiveBayesClassifier.train(train_features)\n",
    "\n",
    "# Evaluate on test features (unseen!)\n",
    "acc = accuracy(classifier, test_features)\n",
    "print(f\"âœ… Fair Accuracy: {acc:.3f}\")  # Should be ~0.70â€“0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cc4099-947b-4da0-ad31-451d18ce0df0",
   "metadata": {},
   "source": [
    "# ðŸ”§ Step 7: Save the Model (So You Can Use It Later!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9861e59f-fb68-46e5-845f-4ce1938631dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "with open(\"movie_sentiment_classifier.pkl\", \"wb\") as f:\n",
    "    pickle.dump(classifier, f)\n",
    "\n",
    "# Save top_100_positive too\n",
    "with open(\"top_100_positive.pkl\", \"wb\") as f:\n",
    "    pickle.dump(top_100_positive, f)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab696e5c-23e2-4149-8c5a-516a759f19b5",
   "metadata": {},
   "source": [
    "âœ… Now itâ€™s saved on your computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4452c6b4-694e-46ac-8415-a9b960a2e707",
   "metadata": {},
   "source": [
    "# ðŸ”§ Step 8: Load and Use Later\n",
    "Later, in a new script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27a326c0-dc82-480e-babe-9c80209c923e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: neg\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from statistics import mean\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load the saved objects\n",
    "with open(\"movie_sentiment_classifier.pkl\", \"rb\") as f:\n",
    "    classifier = pickle.load(f)\n",
    "\n",
    "with open(\"top_100_positive.pkl\", \"rb\") as f:\n",
    "    top_100_positive = pickle.load(f)\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define extract_features again\n",
    "def extract_features(text):\n",
    "    features = {}\n",
    "    wordcount = 0\n",
    "    compound_scores = []\n",
    "    positive_scores = []\n",
    "\n",
    "    for sentence in sent_tokenize(text):\n",
    "        for word in word_tokenize(sentence):\n",
    "            if word.lower() in top_100_positive:\n",
    "                wordcount += 1\n",
    "        compound_scores.append(sia.polarity_scores(sentence)[\"compound\"])\n",
    "        positive_scores.append(sia.polarity_scores(sentence)[\"pos\"])\n",
    "\n",
    "    features[\"mean_compound\"] = mean(compound_scores) + 1\n",
    "    features[\"mean_positive\"] = mean(positive_scores)\n",
    "    features[\"wordcount\"] = wordcount\n",
    "\n",
    "    return features\n",
    "\n",
    "# Classify new review\n",
    "new_review = \"This movie was absolutely bad. I hate it!\"\n",
    "new_features = extract_features(new_review)\n",
    "print(\"Prediction:\", classifier.classify(new_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b2bed6-5827-4af4-b8f5-e44e6f519e05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
