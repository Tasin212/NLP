{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05fbbb53-9993-4441-9579-47a627ae1d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = '''\"I just finished a great book on NLP! ðŸ“– #NLP #DataScience https://example.com/nlp-book\",\"Wow, this is a terrible movie. So boringggg. ðŸ‘ŽðŸ˜ \",\"I don't know what to think about this. There are too many facts, figures and numbers: 1, 2, 3.\",\"The product is amazing!!! It's perfect.\",\"The product is not so good. It is a really bad quality product.\",\"So so, not impressed. So much hype, so little to show for it.\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07d7a8ab-f21c-41ae-9700-ddbd36cd9c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"I just finished a great book on NLP! ðŸ“– #NLP #DataScience  this is a terrible movie. So boringggg. ðŸ‘ŽðŸ˜ \",\"I don\\'t know what to think about this. There are too many facts, figures and numbers: 1, 2, 3.\",\"The product is amazing!!! It\\'s perfect.\",\"The product is not so good. It is a really bad quality product.\",\"So so, not impressed. So much hype, so little to show for it.\"\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove URL\n",
    "import re\n",
    "def remove_url(text):\n",
    "    exp = r'https?://\\S+|www\\.\\S+'\n",
    "    result = re.sub(exp, r'', text)\n",
    "    return result\n",
    "\n",
    "url_remove = remove_url(raw_data)\n",
    "url_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4269046d-25f5-4f84-b195-60154f270aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I just finished a great book on NLP ðŸ“– NLP DataScience  this is a terrible movie So boringggg ðŸ‘ŽðŸ˜ I dont know what to think about this There are too many facts figures and numbers 1 2 3The product is amazing Its perfectThe product is not so good It is a really bad quality productSo so not impressed So much hype so little to show for it\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing punctuation\n",
    "import string\n",
    "exclude = string.punctuation\n",
    "\n",
    "def remove_punc(text):\n",
    "    res = text.translate(str.maketrans('','',exclude))\n",
    "    return res\n",
    "punc_remove = remove_punc(url_remove)\n",
    "punc_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60781e38-c196-49b7-95bc-2bd97063b68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finished great book NLP ðŸ“– NLP DataScience terrible movie boringggg ðŸ‘ŽðŸ˜ I dont know think many facts figures numbers 1 2 3The product amazing perfectThe product good really bad quality productSo impressed much hype little show'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = [word for word in text.split() if word.lower() not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "stopwords_remove = remove_stopwords(punc_remove)\n",
    "stopwords_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cf723f6-479f-41d7-897b-845d014d6ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finished great book NLP  NLP DataScience terrible movie boringggg I dont know think many facts figures numbers 1 2 3The product amazing perfectThe product good really bad quality productSo impressed much hype little show'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing emojis\n",
    "import re\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\" # EMOTICONS\n",
    "                               u\"\\U0001F300-\\U0001F5FF\" # SYMBOLS & PICTOGRAPHS\n",
    "                               u\"\\U0001F680-\\U0001F6FF\" # TRANSPORT & MAP SYMBOLS\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\" # FLAGS (IOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'',text)\n",
    "\n",
    "emoji_remove = remove_emoji(stopwords_remove)\n",
    "emoji_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35aeaf3f-984b-44c9-b91f-bcfee909d3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\labib\\anaconda3\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\labib\\anaconda3\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\labib\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\labib\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\labib\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\labib\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\labib\\anaconda3\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e5fe518-6292-485a-a2fc-92e0bedd0ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1de7b0d-4553-4de1-86ba-f4ea4463d41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finished great book NLP  NLP DataScience terrible movie boringggg I dont know think many facts figures numbers 1 2 The product amazing perfectThe product good really bad quality production impressed much hope little show'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spelling correction\n",
    "txtblb = TextBlob(emoji_remove)\n",
    "spell_correction = txtblb.correct().string\n",
    "spell_correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25387d23-721e-4b09-92d7-6b0f46d8b354",
   "metadata": {},
   "source": [
    "As we can see here TextBlob didn't work properly the word boringgg remains same so we need to use more robous library for spell checking. For more effective spelling correction, especially with non-standard spellings or slang, you need a library with a more advanced algorithm. Two popular and powerful alternatives are pyspellchecker and SymSpell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c786fe1-6976-45d3-b647-b66b7a83bed2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspellchecker\n",
      "  Downloading pyspellchecker-0.8.3-py3-none-any.whl.metadata (9.5 kB)\n",
      "Downloading pyspellchecker-0.8.3-py3-none-any.whl (7.2 MB)\n",
      "   ---------------------------------------- 0.0/7.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/7.2 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.3/7.2 MB 4.1 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.8/7.2 MB 3.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.1/7.2 MB 3.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.9/7.2 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.4/7.2 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.9/7.2 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.5/7.2 MB 2.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 5.0/7.2 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.5/7.2 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.8/7.2 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.6/7.2 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.1/7.2 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.2/7.2 MB 2.6 MB/s eta 0:00:00\n",
      "Installing collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.8.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7bb1e67-9cda-4e8a-af33-a34490148fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd197979-011a-42fa-8b99-22eddbc92f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"finished great book nap nap DataScience terrible movie boringggg I don't know think many facts figures numbers 1 2 the product amazing perfective product good really bad quality products impressed much hype little show\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The TypeError: sequence item 5: expected str instance, NoneType found means that the 6th item (Python lists are 0-indexed) in the list you were \n",
    "# trying to join was None instead of a string.\n",
    "def word_corrected(text):\n",
    "    res = [spell.correction(word) if spell.correction(word) is not None else word for word in text.split()]\n",
    "    return \" \".join(res)\n",
    "\n",
    "corrected_words = word_corrected(emoji_remove)\n",
    "corrected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bce40b6e-1dfd-4cc8-b15c-a03524a1eb96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"finished great book nap nap DataScience terrible movie boring I don't know think many facts figures numbers 1 2 the product amazing perfective product good really bad quality products impressed much hype little show\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#as we can see still it's not yet solved so let's apply regex\n",
    "def normalize_repetitions(text):\n",
    "    # This regex looks for any character (\\w) followed by itself two or more times (\\1{2,})\n",
    "    # It replaces the entire match with a single instance of the character (\\1)\n",
    "    return re.sub(r'(\\w)\\1{2,}', r'\\1', text)\n",
    "    \n",
    "def word_corrected(text):\n",
    "    text = normalize_repetitions(text)\n",
    "    res = [spell.correction(word) if spell.correction(word) is not None else word for word in text.split()]\n",
    "    return \" \".join(res)\n",
    "\n",
    "corrected_words = word_corrected(emoji_remove)\n",
    "corrected_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f670bb-7e0e-4cbd-9e1c-a1d44bab82df",
   "metadata": {},
   "source": [
    "\\1: This is a backreference that refers back to the content of the first capturing group (\\w). In the case of the input boringggg, the first captured character is g. So, \\1 literally means \"the character g.\"\n",
    "In the context of boringggg:\n",
    "The regex engine starts from the beginning.\n",
    "It finds b, o, r, i, n, g. The (\\w) captures the first g.\n",
    "The pattern then looks for at least two more consecutive instances of that same captured character (\\1 which is now g) due to the \\1{2,} part of the regex.\n",
    "The engine finds ggg, which consists of three g's. Since 3 >= 2, this whole sequence matches the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a298af1-9226-4fcd-99a6-957ee38694e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
